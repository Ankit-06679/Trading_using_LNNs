{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa910968",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from torchdiffeq import odeint\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2dc584a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Warning: NaNs/Infs detected in X. Fixing...\n",
      "‚úÖ Data loaded and sanitized\n"
     ]
    }
   ],
   "source": [
    "X = np.load(\"C:\\\\Users\\\\ankit\\\\OneDrive\\\\Desktop\\\\Capstone\\\\trial_05\\\\X_preprocessed.npy\")\n",
    "y = np.load(\"C:\\\\Users\\\\ankit\\\\OneDrive\\\\Desktop\\\\Capstone\\\\trial_05\\\\y_preprocessed.npy\")\n",
    "\n",
    "# Sanitize NaN/Inf in X\n",
    "if np.any(~np.isfinite(X)):\n",
    "    print(\"‚ö†Ô∏è Warning: NaNs/Infs detected in X. Fixing...\")\n",
    "    # forward fill NaNs\n",
    "    for i in range(X.shape[2]):\n",
    "        col = X[:, :, i]\n",
    "        mask = ~np.isfinite(col)\n",
    "        if mask.any():\n",
    "            col[mask] = np.nan\n",
    "            # forward/back fill\n",
    "            col = np.where(np.isnan(col), np.nanmean(col), col)\n",
    "        X[:, :, i] = np.clip(col, -10, 10)\n",
    "\n",
    "# Sanitize NaN/Inf in y\n",
    "if np.any(~np.isfinite(y)):\n",
    "    print(\"‚ö†Ô∏è Warning: NaNs/Infs detected in y. Fixing...\")\n",
    "    y = np.where(np.isfinite(y), y, np.nanmean(y))\n",
    "    y = np.clip(y, -10, 10)\n",
    "\n",
    "print(\"‚úÖ Data loaded and sanitized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed10c77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, shuffle=False\n",
    ")\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).unsqueeze(-1)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val, dtype=torch.float32).unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "222ae395",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LTCCell(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.W = nn.Linear(input_dim + hidden_dim, hidden_dim)\n",
    "        self.time_constant = nn.Parameter(torch.ones(hidden_dim))\n",
    "\n",
    "    def forward(self, t, h):\n",
    "        # h: (batch, hidden_dim)\n",
    "        x = self.current_input  # set externally\n",
    "        combined = torch.cat([x, h], dim=-1)\n",
    "        dh = torch.tanh(self.W(combined)) - h\n",
    "        return dh / (torch.abs(self.time_constant) + 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c78f3af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackedLTC(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim, t_span=[0.0, 1.0]):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.t_span = t_span\n",
    "        prev_dim = input_dim\n",
    "        for hdim in hidden_dims:\n",
    "            self.layers.append(LTCCell(prev_dim, hdim))\n",
    "            prev_dim = hdim\n",
    "        self.head = nn.Linear(prev_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        hs = [torch.zeros(batch_size, layer.hidden_dim, device=device) for layer in self.layers]\n",
    "\n",
    "        for t in range(seq_len):\n",
    "            cur = x[:, t, :]\n",
    "            for li, layer in enumerate(self.layers):\n",
    "                if li == 0:\n",
    "                    layer.current_input = cur\n",
    "                else:\n",
    "                    layer.current_input = hs[li-1]\n",
    "                # ‚úÖ FIX: ensure t_span is tensor\n",
    "                t_span = torch.tensor(self.t_span, dtype=torch.float32, device=cur.device)\n",
    "                h_next = odeint(layer, hs[li], t_span)[-1]\n",
    "                hs[li] = torch.tanh(h_next)\n",
    "\n",
    "        final_h = hs[-1]\n",
    "        out = self.head(final_h)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3f800c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ankit\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = StackedLTC(input_dim=X.shape[2], hidden_dims=[64, 64, 32], output_dim=1).to(DEVICE)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "train_ds = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "val_ds = torch.utils.data.TensorDataset(X_val, y_val)\n",
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_ds, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8501ed44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 | Train Loss: 0.002184 | Val Loss: 0.005731 | Train MSE: 0.002184 | Val MSE: 0.005731 | Train MAE: 0.035644 | Val MAE: 0.057007 | Train R¬≤: 0.938654 | Val R¬≤: 0.751910\n",
      "Epoch 2/20 | Train Loss: 0.001307 | Val Loss: 0.003115 | Train MSE: 0.001307 | Val MSE: 0.003115 | Train MAE: 0.026781 | Val MAE: 0.043140 | Train R¬≤: 0.963271 | Val R¬≤: 0.865147\n",
      "Epoch 3/20 | Train Loss: 0.001013 | Val Loss: 0.002334 | Train MSE: 0.001013 | Val MSE: 0.002334 | Train MAE: 0.023619 | Val MAE: 0.036686 | Train R¬≤: 0.971533 | Val R¬≤: 0.898934\n",
      "Epoch 4/20 | Train Loss: 0.001028 | Val Loss: 0.002289 | Train MSE: 0.001028 | Val MSE: 0.002289 | Train MAE: 0.024937 | Val MAE: 0.036060 | Train R¬≤: 0.971109 | Val R¬≤: 0.900895\n",
      "Epoch 5/20 | Train Loss: 0.000899 | Val Loss: 0.001312 | Train MSE: 0.000899 | Val MSE: 0.001312 | Train MAE: 0.022392 | Val MAE: 0.029300 | Train R¬≤: 0.974743 | Val R¬≤: 0.943203\n",
      "Epoch 6/20 | Train Loss: 0.000924 | Val Loss: 0.002215 | Train MSE: 0.000924 | Val MSE: 0.002215 | Train MAE: 0.023392 | Val MAE: 0.037276 | Train R¬≤: 0.974036 | Val R¬≤: 0.904089\n",
      "Epoch 7/20 | Train Loss: 0.000867 | Val Loss: 0.001041 | Train MSE: 0.000867 | Val MSE: 0.001041 | Train MAE: 0.022705 | Val MAE: 0.024838 | Train R¬≤: 0.975656 | Val R¬≤: 0.954943\n",
      "Epoch 8/20 | Train Loss: 0.000656 | Val Loss: 0.001454 | Train MSE: 0.000656 | Val MSE: 0.001454 | Train MAE: 0.019316 | Val MAE: 0.029159 | Train R¬≤: 0.981571 | Val R¬≤: 0.937055\n",
      "Epoch 9/20 | Train Loss: 0.000608 | Val Loss: 0.000785 | Train MSE: 0.000608 | Val MSE: 0.000785 | Train MAE: 0.018649 | Val MAE: 0.021190 | Train R¬≤: 0.982930 | Val R¬≤: 0.966018\n",
      "Epoch 10/20 | Train Loss: 0.000605 | Val Loss: 0.000660 | Train MSE: 0.000605 | Val MSE: 0.000660 | Train MAE: 0.018884 | Val MAE: 0.019947 | Train R¬≤: 0.983013 | Val R¬≤: 0.971412\n",
      "Epoch 11/20 | Train Loss: 0.000518 | Val Loss: 0.000896 | Train MSE: 0.000518 | Val MSE: 0.000896 | Train MAE: 0.017318 | Val MAE: 0.023067 | Train R¬≤: 0.985436 | Val R¬≤: 0.961215\n",
      "Epoch 12/20 | Train Loss: 0.000511 | Val Loss: 0.000699 | Train MSE: 0.000511 | Val MSE: 0.000699 | Train MAE: 0.017095 | Val MAE: 0.020864 | Train R¬≤: 0.985637 | Val R¬≤: 0.969746\n",
      "Epoch 13/20 | Train Loss: 0.000542 | Val Loss: 0.000595 | Train MSE: 0.000542 | Val MSE: 0.000595 | Train MAE: 0.017830 | Val MAE: 0.018565 | Train R¬≤: 0.984763 | Val R¬≤: 0.974220\n",
      "Epoch 14/20 | Train Loss: 0.000478 | Val Loss: 0.000566 | Train MSE: 0.000478 | Val MSE: 0.000566 | Train MAE: 0.016842 | Val MAE: 0.018170 | Train R¬≤: 0.986559 | Val R¬≤: 0.975485\n",
      "Epoch 15/20 | Train Loss: 0.000483 | Val Loss: 0.000609 | Train MSE: 0.000483 | Val MSE: 0.000609 | Train MAE: 0.016823 | Val MAE: 0.019033 | Train R¬≤: 0.986441 | Val R¬≤: 0.973614\n",
      "Epoch 16/20 | Train Loss: 0.000455 | Val Loss: 0.000555 | Train MSE: 0.000455 | Val MSE: 0.000555 | Train MAE: 0.016282 | Val MAE: 0.017997 | Train R¬≤: 0.987219 | Val R¬≤: 0.975967\n",
      "Epoch 17/20 | Train Loss: 0.000514 | Val Loss: 0.001002 | Train MSE: 0.000514 | Val MSE: 0.001002 | Train MAE: 0.017502 | Val MAE: 0.026453 | Train R¬≤: 0.985557 | Val R¬≤: 0.956604\n",
      "Epoch 18/20 | Train Loss: 0.000460 | Val Loss: 0.000734 | Train MSE: 0.000460 | Val MSE: 0.000734 | Train MAE: 0.016490 | Val MAE: 0.021444 | Train R¬≤: 0.987090 | Val R¬≤: 0.968235\n",
      "Epoch 19/20 | Train Loss: 0.000411 | Val Loss: 0.000427 | Train MSE: 0.000411 | Val MSE: 0.000427 | Train MAE: 0.015381 | Val MAE: 0.015269 | Train R¬≤: 0.988463 | Val R¬≤: 0.981507\n",
      "Epoch 20/20 | Train Loss: 0.000422 | Val Loss: 0.000558 | Train MSE: 0.000422 | Val MSE: 0.000558 | Train MAE: 0.015471 | Val MAE: 0.017967 | Train R¬≤: 0.988159 | Val R¬≤: 0.975860\n",
      "‚úÖ Training finished. Final model saved to lnn_final_model.pth\n",
      "\n",
      "üìä Final Training Results:\n",
      "    Epoch  Train Loss  Val Loss  Train MSE   Val MSE  Train MAE   Val MAE  \\\n",
      "0       1    0.002184  0.005731   0.002184  0.005731   0.035644  0.057007   \n",
      "1       2    0.001307  0.003115   0.001307  0.003115   0.026781  0.043140   \n",
      "2       3    0.001013  0.002334   0.001013  0.002334   0.023619  0.036686   \n",
      "3       4    0.001028  0.002289   0.001028  0.002289   0.024937  0.036060   \n",
      "4       5    0.000899  0.001312   0.000899  0.001312   0.022392  0.029300   \n",
      "5       6    0.000924  0.002215   0.000924  0.002215   0.023392  0.037276   \n",
      "6       7    0.000867  0.001041   0.000867  0.001041   0.022705  0.024838   \n",
      "7       8    0.000656  0.001454   0.000656  0.001454   0.019316  0.029159   \n",
      "8       9    0.000608  0.000785   0.000608  0.000785   0.018649  0.021190   \n",
      "9      10    0.000605  0.000660   0.000605  0.000660   0.018884  0.019947   \n",
      "10     11    0.000518  0.000896   0.000518  0.000896   0.017318  0.023067   \n",
      "11     12    0.000511  0.000699   0.000511  0.000699   0.017095  0.020864   \n",
      "12     13    0.000542  0.000595   0.000542  0.000595   0.017830  0.018565   \n",
      "13     14    0.000478  0.000566   0.000478  0.000566   0.016842  0.018170   \n",
      "14     15    0.000483  0.000609   0.000483  0.000609   0.016823  0.019033   \n",
      "15     16    0.000455  0.000555   0.000455  0.000555   0.016282  0.017997   \n",
      "16     17    0.000514  0.001002   0.000514  0.001002   0.017502  0.026453   \n",
      "17     18    0.000460  0.000734   0.000460  0.000734   0.016490  0.021444   \n",
      "18     19    0.000411  0.000427   0.000411  0.000427   0.015381  0.015269   \n",
      "19     20    0.000422  0.000558   0.000422  0.000558   0.015471  0.017967   \n",
      "\n",
      "    Train R¬≤    Val R¬≤  \n",
      "0   0.938654  0.751910  \n",
      "1   0.963271  0.865147  \n",
      "2   0.971533  0.898934  \n",
      "3   0.971109  0.900895  \n",
      "4   0.974743  0.943203  \n",
      "5   0.974036  0.904089  \n",
      "6   0.975656  0.954943  \n",
      "7   0.981571  0.937055  \n",
      "8   0.982930  0.966018  \n",
      "9   0.983013  0.971412  \n",
      "10  0.985436  0.961215  \n",
      "11  0.985637  0.969746  \n",
      "12  0.984763  0.974220  \n",
      "13  0.986559  0.975485  \n",
      "14  0.986441  0.973614  \n",
      "15  0.987219  0.975967  \n",
      "16  0.985557  0.956604  \n",
      "17  0.987090  0.968235  \n",
      "18  0.988463  0.981507  \n",
      "19  0.988159  0.975860  \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torchmetrics import MeanAbsoluteError, MeanSquaredError, R2Score\n",
    "\n",
    "EPOCHS = 20\n",
    "final_model_path = \"lnn_final_model.pth\"\n",
    "\n",
    "# üìä Store results for each epoch\n",
    "history = {\n",
    "    \"Epoch\": [],\n",
    "    \"Train Loss\": [],\n",
    "    \"Val Loss\": [],\n",
    "    \"Train MSE\": [],\n",
    "    \"Val MSE\": [],\n",
    "    \"Train MAE\": [],\n",
    "    \"Val MAE\": [],\n",
    "    \"Train R¬≤\": [],\n",
    "    \"Val R¬≤\": []\n",
    "}\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    # Train metrics\n",
    "    train_mse = MeanSquaredError().to(DEVICE)\n",
    "    train_mae = MeanAbsoluteError().to(DEVICE)\n",
    "    train_r2  = R2Score().to(DEVICE)\n",
    "\n",
    "    for Xb, yb in train_loader:\n",
    "        Xb, yb = Xb.to(DEVICE), yb.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(Xb)\n",
    "        loss = criterion(out, yb)\n",
    "        if torch.isnan(loss):\n",
    "            print(\"‚ùå NaN loss detected! Breaking...\")\n",
    "            break\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * Xb.size(0)\n",
    "        \n",
    "        train_mse.update(out, yb)\n",
    "        train_mae.update(out, yb)\n",
    "        train_r2.update(out, yb)\n",
    "    \n",
    "    train_loss = train_loss / len(train_ds)\n",
    "    train_mse_val = train_mse.compute().item()\n",
    "    train_mae_val = train_mae.compute().item()\n",
    "    train_r2_val  = train_r2.compute().item()\n",
    "\n",
    "    # -----------------------\n",
    "    # Validation\n",
    "    # -----------------------\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    \n",
    "    val_mse = MeanSquaredError().to(DEVICE)\n",
    "    val_mae = MeanAbsoluteError().to(DEVICE)\n",
    "    val_r2  = R2Score().to(DEVICE)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for Xb, yb in val_loader:\n",
    "            Xb, yb = Xb.to(DEVICE), yb.to(DEVICE)\n",
    "            out = model(Xb)\n",
    "            loss = criterion(out, yb)\n",
    "            val_loss += loss.item() * Xb.size(0)\n",
    "            \n",
    "            val_mse.update(out, yb)\n",
    "            val_mae.update(out, yb)\n",
    "            val_r2.update(out, yb)\n",
    "    \n",
    "    val_loss = val_loss / len(val_ds)\n",
    "    val_mse_val = val_mse.compute().item()\n",
    "    val_mae_val = val_mae.compute().item()\n",
    "    val_r2_val  = val_r2.compute().item()\n",
    "\n",
    "    # -----------------------\n",
    "    # Save results\n",
    "    # -----------------------\n",
    "    history[\"Epoch\"].append(epoch)\n",
    "    history[\"Train Loss\"].append(train_loss)\n",
    "    history[\"Val Loss\"].append(val_loss)\n",
    "    history[\"Train MSE\"].append(train_mse_val)\n",
    "    history[\"Val MSE\"].append(val_mse_val)\n",
    "    history[\"Train MAE\"].append(train_mae_val)\n",
    "    history[\"Val MAE\"].append(val_mae_val)\n",
    "    history[\"Train R¬≤\"].append(train_r2_val)\n",
    "    history[\"Val R¬≤\"].append(val_r2_val)\n",
    "\n",
    "    print(f\"Epoch {epoch}/{EPOCHS} | \"\n",
    "          f\"Train Loss: {train_loss:.6f} | Val Loss: {val_loss:.6f} | \"\n",
    "          f\"Train MSE: {train_mse_val:.6f} | Val MSE: {val_mse_val:.6f} | \"\n",
    "          f\"Train MAE: {train_mae_val:.6f} | Val MAE: {val_mae_val:.6f} | \"\n",
    "          f\"Train R¬≤: {train_r2_val:.6f} | Val R¬≤: {val_r2_val:.6f}\")\n",
    "\n",
    "# -----------------------\n",
    "# Save model AFTER training\n",
    "# -----------------------\n",
    "torch.save(model.state_dict(), final_model_path)\n",
    "print(f\"‚úÖ Training finished. Final model saved to {final_model_path}\")\n",
    "\n",
    "# -----------------------\n",
    "# üìä Tabulate results\n",
    "# -----------------------\n",
    "results_df = pd.DataFrame(history)\n",
    "print(\"\\nüìä Final Training Results:\")\n",
    "print(results_df)\n",
    "\n",
    "# Optionally save to CSV/Excel\n",
    "results_df.to_csv(\"training_results.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f364062d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StackedLTC(\n",
       "  (layers): ModuleList(\n",
       "    (0): LTCCell(\n",
       "      (W): Linear(in_features=75, out_features=64, bias=True)\n",
       "    )\n",
       "    (1): LTCCell(\n",
       "      (W): Linear(in_features=128, out_features=64, bias=True)\n",
       "    )\n",
       "    (2): LTCCell(\n",
       "      (W): Linear(in_features=96, out_features=32, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (head): Linear(in_features=32, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = StackedLTC(input_dim=X.shape[2], hidden_dims=[64, 64, 32], output_dim=1).to(DEVICE)\n",
    "model.load_state_dict(torch.load(\"C:\\\\Users\\\\ankit\\\\OneDrive\\\\Desktop\\\\Capstone\\\\trial_05\\\\lnn_final_model.pth\", map_location=DEVICE))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46e83f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5c8e7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:\\\\Users\\\\ankit\\\\OneDrive\\\\Desktop\\\\Capstone\\\\trial_05\\\\TCS_2020_present.csv\")  \n",
    "# Assume df has columns: Date, Open, High, Low, Close, Volume\n",
    "\n",
    "# Feature preparation (adjust same way you preprocessed X_preprocessed.npy)\n",
    "# For demo, we use last N candles as input sequence\n",
    "SEQ_LEN = 30\n",
    "features = [\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]\n",
    "X, y = [], []"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
